"""
Download the Copernicus Global Land Service Lake Water Quality NetCDF files,
crop and convert to Cloud Optimized Geotiffs, and push to an S3 bucket.
"""

import json
import os
import posixpath
import sys
import warnings
from datetime import datetime

import click
import numpy as np
import rioxarray  # noqa F401
from odc.geo.xr import assign_crs
from rasterio.errors import NotGeoreferencedWarning
from tqdm import tqdm

from deafrica.data.cgls_lwq.constants import MANIFEST_FILE_URLS
from deafrica.data.cgls_lwq.netcdf import (
    get_netcdf_subdatasets_uris,
    get_netcdf_urls_from_manifest,
    parse_netcdf_subdatasets_uri,
    parse_netcdf_url,
    read_netcdf_url,
)
from deafrica.data.cgls_lwq.tiles import (
    get_africa_tiles,
    get_tile_index_str,
    get_tile_index_str_tuple,
)
from deafrica.io import (
    check_directory_exists,
    check_file_exists,
    download_file_from_url,
    get_filesystem,
    is_local_path,
    join_url,
)
from deafrica.logs import setup_logging

# Suppress the warning
warnings.filterwarnings("ignore", category=NotGeoreferencedWarning)


def get_expected_cog_url(
    output_dir: str,
    source_netcdf_url: str,
    measurement_name: str,
    tile_index: tuple[int, int],
) -> str:
    """
    Get the expected file path for a COG file based on the output directory, source netcdf
    url, measurement name and tile index.

    Parameters
    ----------
    output_dir : str
        Directory to write the COG file to.
    source_netcdf_url : str
        URL for the CGLS Lake Water Quality NetCDF file the COG is derived from.
    measurement_name : str
        Name of the measurement for the COG file.
    tile_index : tuple[int, int]
        Tile index of the COG file.

    Returns
    -------
    str
        COG output file path.
    """
    filename_prefix, acronym, date_str, area, sensor, version, _ = parse_netcdf_url(
        source_netcdf_url
    )
    date = datetime.strptime(date_str, "%Y%m%d%H%M%S")
    year = str(date.year)
    month = f"{date.month:02d}"
    day = f"{date.day:02d}"

    tile_index_str = get_tile_index_str(tile_index)
    tile_index_str_x, tile_index_str_y = get_tile_index_str_tuple(tile_index_str)
    parent_dir = join_url(
        output_dir,
        tile_index_str_x,
        tile_index_str_y,
        year,
        month,
        day,
    )
    file_name = f"{filename_prefix}_{acronym}_{date_str}_{area}_{sensor}_{version}_{tile_index_str}_{measurement_name}.tif"

    output_cog_url = join_url(parent_dir, file_name)
    return output_cog_url


def get_output_cog_url(
    output_dir: str,
    netcdf_subdataset_uri: str,
    tile_index: tuple[int, int],
) -> str:
    """
    Create output file path for COG generated by cropping CGLS LWQ netcdf subdataset
    to tile bounds.

    Parameters
    ----------
    output_dir : str
        Directory to write the CoG file to.
    netcdf_subdataset_uri : str
        CGLS LWQ netcdf subdataset uri to generate CoG file from.
    tile_index : tuple[int, int]
        Tile index of tile to crop the CGLS LWQ netcdf subdataset to, by default None

    Returns
    -------
    str
        CoG output file path.
    """
    _, _, netcdf_url, subdataset_variable = parse_netcdf_subdatasets_uri(
        netcdf_subdataset_uri
    )

    output_cog_url = get_expected_cog_url(
        output_dir=output_dir,
        source_netcdf_url=netcdf_url,
        measurement_name=subdataset_variable,
        tile_index=tile_index,
    )

    if is_local_path(output_cog_url):
        parent_dir = os.path.dirname(output_cog_url)
    else:
        parent_dir = posixpath.dirname(output_cog_url)

    if not check_directory_exists(parent_dir):
        fs = get_filesystem(parent_dir, anon=False)
        fs.makedirs(parent_dir, exist_ok=True)

    return output_cog_url


@click.command(
    "download-cgls-lwq-cogs",
    help="Download the Copernicus Global Land Service Lake Water Quality datasets,"
    "crop and convert to Cloud Optimized Geotiffs, and push to an S3 bucket.",
    no_args_is_help=True,
)
@click.option(
    "--product-name",
    type=click.Choice(list(MANIFEST_FILE_URLS.keys()), case_sensitive=True),
    help="Name of the product to generate the stac item files for",
)
@click.option(
    "--cog-output-dir",
    type=str,
    help="Directory to write the cog files to",
)
@click.option("--overwrite/--no-overwrite", default=False, show_default=True)
@click.option(
    "--max-parallel-steps",
    default=1,
    show_default=True,
    type=int,
    help="Maximum number of parallel steps/pods to have in the workflow.",
)
@click.option(
    "--worker-idx",
    default=0,
    show_default=True,
    type=int,
    help="Sequential index which will be used to define the range of geotiffs the pod will work with.",
)
@click.option(
    "--url-filter",
    default=None,
    show_default=True,
    type=str,
    help="Filter to select netcdf urls to download cogs for.",
)
def download_cogs(
    product_name: str,
    cog_output_dir: str,
    overwrite: bool,
    max_parallel_steps: int,
    worker_idx: int,
    url_filter: str,
):
    # Setup logging level
    log = setup_logging()

    if product_name not in MANIFEST_FILE_URLS.keys():
        raise NotImplementedError(
            f"Manifest file url not configured for the product {product_name}"
        )

    # Read urls available for the product
    all_netcdf_urls = get_netcdf_urls_from_manifest(MANIFEST_FILE_URLS[product_name])
    log.info(f"Found {len(all_netcdf_urls)} netcdf urls in the manifest file")

    # Apply filter
    if url_filter:
        all_netcdf_urls = [i for i in all_netcdf_urls if url_filter in i]
        if len(all_netcdf_urls) < 1:
            raise ValueError(
                f"No netcdf urls found in manifest file that match the filter '{url_filter}'"
            )
        else:
            log.info(
                f"Found {len(all_netcdf_urls)} netcdf urls in the manifest file that match the filter '{url_filter}'"
            )

    # Split files equally among the workers
    task_chunks = np.array_split(np.array(all_netcdf_urls), max_parallel_steps)
    task_chunks = [chunk.tolist() for chunk in task_chunks]
    task_chunks = list(filter(None, task_chunks))

    # In case of the index being bigger than the number of positions in the array, the extra POD isn't necessary
    if len(task_chunks) <= worker_idx:
        log.warning(f"Worker {worker_idx} Skipped!")
        sys.exit(0)

    log.info(f"Executing worker {worker_idx}")

    netcdf_urls = task_chunks[worker_idx]
    log.info(f"Worker {worker_idx} to process {len(netcdf_urls)} netcdf files.")

    # Define the tiles over Africa
    if "300m" in netcdf_urls[0]:
        grid_res = 300
    elif "100m" in netcdf_urls[0]:
        grid_res = 100

    tiles = get_africa_tiles(grid_res)

    tmp_dir = f"tmp/{product_name}/netcdfs/"
    failed_tasks = []
    max_retries = 5
    for idx, netcdf_url in enumerate(netcdf_urls):
        log.info(f"Processing {netcdf_url} {idx + 1}/{len(netcdf_urls)}")
        # Download file instead.
        output_netcdf_file_path = join_url(tmp_dir, posixpath.basename(netcdf_url))
        log.info(f"Downloading {netcdf_url} to {output_netcdf_file_path}")

        if check_file_exists(output_netcdf_file_path):
            log.info(
                f"{output_netcdf_file_path} already exists! Skippping download ..."
            )
        else:
            try:
                output_netcdf_file_path = download_file_from_url(
                    url=netcdf_url, output_file_path=output_netcdf_file_path, chunks=100
                )
                log.info("Download complete!")
            except Exception as error:
                log.exception(error)
                log.error(f"Failed to download {netcdf_url}")
                failed_tasks.append(f"Failed to download {netcdf_url}")
                continue

        if check_file_exists(output_netcdf_file_path):
            log.info(f"Generating cog files for {output_netcdf_file_path}")
            try:
                # Get the subdatasets in the netcdf
                netcdf_subdatasets_uris = get_netcdf_subdatasets_uris(
                    output_netcdf_file_path
                )

                for var, subdataset_uri in netcdf_subdatasets_uris.items():
                    # da = rioxarray.open_rasterio(subdataset_uri).squeeze()
                    da = read_netcdf_url(subdataset_uri, max_retries=max_retries)
                    da = da.squeeze()

                    if "spatial_ref" in list(da.coords):
                        crs_coord_name = "spatial_ref"
                    elif "crs" in list(da.coords):
                        crs_coord_name = "crs"

                    crs = da.rio.crs

                    if crs is None:
                        # Assumption drawn from product manual is
                        # data is either in EPSG:4326 or OGC:CRS84
                        if da.dims[0] in ["y", "lat", "latitude"]:
                            crs = "EPSG:4326"
                        elif da.dims[0] in ["x", "lon", "longitude"]:
                            crs = "OGC:CRS84"

                    da = assign_crs(da, crs, crs_coord_name=crs_coord_name)

                    # Get attributes to be used in tiled COGs
                    attrs = da.attrs
                    exclude = [
                        "lon#",
                        "lat#",
                        "number_of_regions",
                        "TileSize",
                        "NETCDF_",
                        "coordinates",
                    ]
                    filtered_attrs = {
                        k: v
                        for k, v in attrs.items()
                        if not any(sub.lower() in k.lower() for sub in exclude)
                    }
                    da.attrs = filtered_attrs

                    # Crop the netcdf subdataset to each tile
                    with tqdm(
                        iterable=tiles,
                        desc=f"Cropping {var} subdataset",
                        total=len(tiles),
                    ) as tiles:
                        for tile in tiles:
                            tile_idx, tile_geobox = tile
                            output_cog_url = get_output_cog_url(
                                cog_output_dir, subdataset_uri, tile_idx
                            )
                            if not overwrite:
                                if check_file_exists(output_cog_url):
                                    continue

                            cropped_da = da.odc.crop(
                                tile_geobox.extent.to_crs(da.odc.geobox.crs)
                            )

                            # Write cog files
                            if is_local_path(output_cog_url):
                                cropped_da.odc.write_cog(
                                    fname=output_cog_url,
                                    overwrite=True,
                                    tags=filtered_attrs,
                                )
                            else:
                                cog_bytes = cropped_da.odc.write_cog(
                                    fname=":mem:", overwrite=True, tags=filtered_attrs
                                )
                                fs = get_filesystem(output_cog_url, anon=False)
                                with fs.open(output_cog_url, "wb") as f:
                                    f.write(cog_bytes)

                    log.info(f"Written COGs for {var} subdataset")
            except Exception as error:
                log.exception(error)
                log.error(
                    f"Failed to generate cogs for the netcdf {output_netcdf_file_path}"
                )
                failed_tasks.append(
                    f"Failed to generate cogs for the netcdf {output_netcdf_file_path}"
                )
            # Once done remove the file to save on storage in volume
            os.remove(output_netcdf_file_path)
            log.info(f"Deleted {output_netcdf_file_path}")
        else:
            error = f"File {output_netcdf_file_path} downloaded from {netcdf_url} but not detected on file system!"
            log.error(error)
            failed_tasks.append(error)
            continue

    if failed_tasks:
        failed_tasks_json_array = json.dumps(failed_tasks)

        tasks_directory = "/tmp/"
        failed_tasks_output_file = join_url(tasks_directory, "failed_tasks")

        fs = get_filesystem(path=tasks_directory, anon=False)

        if not check_directory_exists(path=tasks_directory):
            fs.mkdirs(path=tasks_directory, exist_ok=True)
            log.info(f"Created directory {tasks_directory}")

        with fs.open(failed_tasks_output_file, "a") as file:
            file.write(failed_tasks_json_array + "\n")
        log.info(f"Failed tasks written to {failed_tasks_output_file}")

        raise RuntimeError(f"{len(failed_tasks)} tasks failed")
